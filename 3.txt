import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler
from datasets import load_dataset
from torch.utils.data import DataLoader
from sklearn.metrics import mean_squared_error

# Load the dataset
# Replace 'imdb' with your dataset
dataset = load_dataset('imdb', split='train[:1000]')  # Using a subset for demonstration

# Tokenize the dataset
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Convert labels to float for regression
def convert_labels(examples):
    examples['label'] = float(examples['label'])
    return examples

tokenized_datasets = tokenized_datasets.map(convert_labels)

# Rename 'label' to 'labels' for compatibility with Hugging Face models
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# Prepare the data loader
train_dataloader = DataLoader(tokenized_datasets, batch_size=16, shuffle=True)

# Load the model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)

# Define the optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)
num_training_steps = len(train_dataloader) * 3  # 3 epochs
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# Define the loss function
loss_fn = torch.nn.MSELoss()

# Training loop
model.train()
for epoch in range(3):  # 3 epochs
    for batch in train_dataloader:
        batch = {k: v.to(model.device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = loss_fn(outputs.logits.squeeze(), batch["labels"])
        
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
    
    print(f"Epoch {epoch + 1} - Loss: {loss.item()}")

# Evaluation (optional)
model.eval()
predictions = []
labels = []

for batch in train_dataloader:
    batch = {k: v.to(model.device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    predictions.extend(outputs.logits.squeeze().cpu().numpy())
    labels.extend(batch["labels"].cpu().numpy())

mse = mean_squared_error(labels, predictions)
print(f"Mean Squared Error: {mse}")
